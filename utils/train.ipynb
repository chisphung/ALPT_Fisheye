{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d6ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision opencv-python numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09efba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# p-norm SE Module [cite: 241]\n",
    "class PNormSE(nn.Module):\n",
    "    def __init__(self, channels, reduction=4):\n",
    "        super(PNormSE, self).__init__()\n",
    "        # Excitation step [cite: 259]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels // reduction, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, kernel_size=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, c, _, _ = x.size()\n",
    "        \n",
    "        # Squeeze step: 1-norm (avg pool) and infinity-norm (max pool) [cite: 253, 254]\n",
    "        avg_pool = F.adaptive_avg_pool2d(x, 1)\n",
    "        max_pool = F.adaptive_max_pool2d(x, 1)\n",
    "        \n",
    "        # Concatenate features [cite: 258, 259]\n",
    "        y = torch.cat([avg_pool, max_pool], dim=1)\n",
    "        \n",
    "        y = self.fc(y)\n",
    "        return x * y\n",
    "\n",
    "# Integration Block (IB) [cite: 239]\n",
    "class IntegrationBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(IntegrationBlock, self).__init__()\n",
    "        self.residual_branch = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        self.p_norm_se = PNormSE(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.residual_branch(x)\n",
    "        res = self.p_norm_se(res)\n",
    "        return F.relu(res + self.shortcut(x))\n",
    "\n",
    "# VertexNet Model\n",
    "class VertexNet(nn.Module):\n",
    "    def __init__(self, num_anchors=9, num_classes=2):\n",
    "        super(VertexNet, self).__init__()\n",
    "        \n",
    "        # Backbone Network [cite: 224]\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.stage2 = self._make_stage(128, 128, num_blocks=2, stride=2)\n",
    "        self.stage3 = self._make_stage(128, 128, num_blocks=2, stride=2)\n",
    "        self.stage4 = self._make_stage(128, 128, num_blocks=2, stride=2)\n",
    "        self.stage5 = self._make_stage(128, 128, num_blocks=2, stride=2)\n",
    "        self.stage6 = self._make_stage(128, 128, num_blocks=2, stride=2)\n",
    "\n",
    "        # Fusion Network (FPN-like) [cite: 271]\n",
    "        self.p5_fusion = self._make_fusion_layer(128, 128)\n",
    "        self.p4_fusion = self._make_fusion_layer(128, 128)\n",
    "        self.p3_fusion = self._make_fusion_layer(128, 128)\n",
    "\n",
    "        # Head Network [cite: 278, 279]\n",
    "        output_channels = num_anchors * (num_classes + 4 + 8) # 2 scores, 4 box offsets, 8 vertex offsets\n",
    "        self.shared_head_A = nn.Conv2d(128, output_channels, kernel_size=3, padding=1)\n",
    "        self.shared_head_B = nn.Conv2d(128, output_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "    def _make_stage(self, in_channels, out_channels, num_blocks, stride):\n",
    "        layers = [IntegrationBlock(in_channels, out_channels, stride)]\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(IntegrationBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_fusion_layer(self, ch1, ch2):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(ch1 + ch2, 128, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Backbone\n",
    "        c1 = self.stage1(x)\n",
    "        c2 = self.stage2(c1)\n",
    "        c3 = self.stage3(c2) # Output for fusion P3\n",
    "        c4 = self.stage4(c3) # Output for fusion P4\n",
    "        c5 = self.stage5(c4) # Output for fusion P5\n",
    "        c6 = self.stage6(c5) # Output for head\n",
    "\n",
    "        # Fusion\n",
    "        p5 = self.p5_fusion(torch.cat([c5, F.interpolate(c6, scale_factor=2, mode='bilinear')], dim=1))\n",
    "        p4 = self.p4_fusion(torch.cat([c4, F.interpolate(p5, scale_factor=2, mode='bilinear')], dim=1))\n",
    "        p3 = self.p3_fusion(torch.cat([c3, F.interpolate(p4, scale_factor=2, mode='bilinear')], dim=1))\n",
    "\n",
    "        # Head\n",
    "        pred_p3 = self.shared_head_A(p3)\n",
    "        pred_p4 = self.shared_head_A(p4)\n",
    "        pred_p5 = self.shared_head_B(p5)\n",
    "        pred_c6 = self.shared_head_B(c6)\n",
    "\n",
    "        return [pred_p3, pred_p4, pred_p5, pred_c6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simplified residual block for SCR-Net\n",
    "class SCRResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(SCRResBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=(1, 3), padding=(0, 1), bias=False),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=(1, 3), padding=(0, 1), bias=False),\n",
    "            nn.BatchNorm2d(channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(x + self.conv(x))\n",
    "\n",
    "class SCRNet(nn.Module):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(SCRNet, self).__init__()\n",
    "        \n",
    "        # Based on architecture in Table II [cite: 319]\n",
    "        self.s_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.s_stage1 = self._make_scr_stage(16, 93, num_blocks=4)\n",
    "        self.s_stage2 = self._make_scr_stage(93, 176, num_blocks=4, stride=2)\n",
    "        self.s_stage3 = self._make_scr_stage(176, 256, num_blocks=4, stride=2)\n",
    "        \n",
    "        # Horizontal Encoding [cite: 332]\n",
    "        self.s_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Squeeze feature maps from 8x32 to 1x32 [cite: 332]\n",
    "            nn.Conv2d(256, 256, kernel_size=(8, 1), stride=(1, 1), padding=0, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.s_stage4 = nn.Sequential(\n",
    "            SCRResBlock(256), SCRResBlock(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            SCRResBlock(256), SCRResBlock(256),\n",
    "            nn.Conv2d(256, 256, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Weight-Sharing Classifier [cite: 351, 354]\n",
    "        # For CCPD, the paper mentions 3 classifiers. For simplicity,\n",
    "        # we combine them into one final classification layer.\n",
    "        # The output size depends on the dataset's character set and length.\n",
    "        self.classifier = nn.Linear(256 * 32, num_outputs)\n",
    "\n",
    "    def _make_scr_stage(self, in_channels, out_channels, num_blocks, stride=1):\n",
    "        layers = []\n",
    "        # Simplified block based on Table II description\n",
    "        # A full implementation would use the Dynamic Regularization described [cite: 316]\n",
    "        for _ in range(num_blocks):\n",
    "            layers.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1))\n",
    "        \n",
    "        if stride != 1:\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.s_conv1(x)   # Output: 32x128\n",
    "        # Stages are simplified here for clarity\n",
    "        # x = self.s_stage1(x) # Output: 32x128\n",
    "        # x = self.s_stage2(x) # Output: 16x64\n",
    "        x = F.adaptive_avg_pool2d(x, (8, 32)) # Simulate stages for shape\n",
    "        x = self.s_stage3(x) # Output: 8x32\n",
    "        \n",
    "        x = self.s_conv2(x)   # Output: 1x32\n",
    "        x = self.s_stage4(x)   # Output: 1x32\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # Flatten for classifier\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6abcebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def rectify_plate(image, vertices):\n",
    "    \"\"\"\n",
    "    Rectifies the license plate using perspective transformation. [cite: 311]\n",
    "    Args:\n",
    "        image: The original high-resolution image.\n",
    "        vertices: A 4x2 numpy array of the LP's corner coordinates.\n",
    "    Returns:\n",
    "        A rectified 256x64 image of the license plate.\n",
    "    \"\"\"\n",
    "    # Ensure vertices are in a consistent order (e.g., top-left, top-right, bottom-right, bottom-left)\n",
    "    # This might require sorting based on coordinates\n",
    "    \n",
    "    # Destination points for the 256x64 rectified image\n",
    "    dst_pts = np.array([[0, 0], [255, 0], [255, 63], [0, 63]], dtype=np.float32)\n",
    "    \n",
    "    # Get the perspective transformation matrix\n",
    "    transform_matrix = cv2.getPerspectiveTransform(vertices.astype(np.float32), dst_pts)\n",
    "    \n",
    "    # Apply the transformation\n",
    "    warped_plate = cv2.warpPerspective(image, transform_matrix, (256, 64))\n",
    "    \n",
    "    return warped_plate\n",
    "\n",
    "def run_inference(image_path, vertexnet_model, scrnet_model):\n",
    "    \"\"\"\n",
    "    Runs the full ALPR pipeline on a single image.\n",
    "    \"\"\"\n",
    "    original_image = cv2.imread(image_path)\n",
    "    if original_image is None:\n",
    "        print(\"Error: Could not load image.\")\n",
    "        return\n",
    "\n",
    "    # 1. Resize for VertexNet \n",
    "    h, w, _ = original_image.shape\n",
    "    resized_image = cv2.resize(original_image, (256, 256))\n",
    "    input_tensor = torch.from_numpy(resized_image.transpose(2, 0, 1)).float().unsqueeze(0) / 255.0\n",
    "\n",
    "    # 2. VertexNet Prediction\n",
    "    vertexnet_model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = vertexnet_model(input_tensor)\n",
    "        # Process predictions: apply NMS, find the best bounding box and vertices\n",
    "        # This is a complex step involving decoding anchor boxes\n",
    "        # For simplicity, we'll assume we have the vertices\n",
    "        # Example vertices (normalized to 256x256)\n",
    "        norm_vertices = np.array([[50, 100], [150, 100], [150, 130], [50, 130]])\n",
    "\n",
    "    # 3. Resample and Rectify [cite: 61]\n",
    "    # Scale vertices back to original image size\n",
    "    original_vertices = norm_vertices * np.array([w / 256.0, h / 256.0])\n",
    "    rectified_lp = rectify_plate(original_image, original_vertices)\n",
    "    \n",
    "    # Prepare for SCR-Net\n",
    "    lp_tensor = torch.from_numpy(rectified_lp.transpose(2, 0, 1)).float().unsqueeze(0) / 255.0\n",
    "    \n",
    "    # 4. SCR-Net Recognition\n",
    "    scrnet_model.eval()\n",
    "    with torch.no_grad():\n",
    "        char_predictions = scrnet_model(lp_tensor)\n",
    "        # Decode the output tensor into a character string\n",
    "        # e.g., by taking argmax for each character position\n",
    "        plate_text = \"DECODED_PLATE\" # Placeholder\n",
    "        \n",
    "    print(f\"Detected License Plate: {plate_text}\")\n",
    "    \n",
    "    cv2.imshow(\"Original Image\", original_image)\n",
    "    cv2.imshow(\"Rectified LP\", rectified_lp)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage (requires trained model weights)\n",
    "# vertexnet = VertexNet()\n",
    "# vertexnet.load_state_dict(torch.load('vertexnet.pth'))\n",
    "# scrnet = SCRNet(num_outputs=234) # 234 for CCPD dataset [cite: 364]\n",
    "# scrnet.load_state_dict(torch.load('scrnet.pth'))\n",
    "# run_inference('path/to/your/car_image.jpg', vertexnet, scrnet)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
